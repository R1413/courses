Midterm Information 
=

**Time**/**Date**: 15. March at 5:45-7:00pm in ECCS 1B12  

Overview and Rules  
--------
- The exam covers everything we have done in class up to and including support vector machines.  This includes all material presented in videos, in-class discussion, in-class exercises, and material introduced in homework, **EXCEPT** 
	- Material on solving for regularized regression weights in the Lecture 9 in-class notebook 
- You are allowed one 8-1/2 x 11in sheet of **handwritten** notes (both sides).  No magnifying glasses! 
- You may use a calculator provided that it cannot access the internet or store large amounts of data. 
- The exam will be a mixture of multiple choice questions and free-response questions in which you may be asked to work through simple examples of algorithms or proofs. 


Material Overview 
---

**General**
- differences between classification and regression 
- differences between supervised and unsupervised learning 
- basic probability 

**K-Nearest Neighbors**
- how the algorithm works 
- how to perform classification with the algorithm 
- properties of the algorithm 
- don't need to know about data structure

**Naive Bayes**
- assumptions behind the algorithm 
- how to compute probabilities from training data 
- how to perform classification with the algorithm 
- Laplace smoothing and it's variants 

**Logistic Regression**
- assumptions behind the algorithm 
- its probabilistic interpretation 
- how to perform classification with the algorithm 
- how to find the weights using Stochastic Gradient Ascent 
- how to efficiently perform regularization in the context of SGA 


<!---
**Decision Trees** 
- general properties of the algorithm 
- entropy, impurity, and information gain 
- how to choose the best split 

**Boosting**
- properties of the AdaBoost algorithm 
- the concept of a weak learner 
- how weights are calculated in AdaBoost 
- how predictions are made from an AdaBoost model 
-->

**Multi-class Classification**
- One-vs-All classification 
- All-Pairs classification 
- Error Correcting Output Codes 

**Model Validation and Evaluation Metrics**
- K-Folds cross-validation 
- Confusion matrices
- TPR vs FPR 
- ROC curves 
- Residual plots 

**Feature Engineering** 
- Text models (BOW and tfidf)
- Residual plots 

**Regression**
- probabilistic interpretation of regression models 
- generalities of how weights are estimated 
- interpretation of the weights in regression models 

**Regularization** 
- the point of regularization 
- Ridge (L2) Regression 
- LASSO (L1) Regression 
- Ridge vs LASSO regression 
- the effect on bias/variance 

**Learning Theory**
- the Bias/Variance Trade-Off
- overfitting, generalization, etc 
- finite vs infinite hypothesis classes 
- PAC Learnability: what it means, what is involved, simple bounds (but no proofs) 
- VC Dimension: how to compute it (possibly simple proofs), why it matters 

**Support Vector Machines** 
- the concept of a margin 
- the geometry behind SVM 
- the workings of hard-margin SVM 
- the workings of soft-margin SVM 
- what a support vector is 
- ideas related to different kernels 


